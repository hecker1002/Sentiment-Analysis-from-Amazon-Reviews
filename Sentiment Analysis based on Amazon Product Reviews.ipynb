{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc1e75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee71f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('D:\\ML\\Cell_Phones_and_Accessories_5.json' , lines = True)\n",
    "\n",
    "# Link to the Dataset: http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d8319a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A30TL5EWN6DFXT</td>\n",
       "      <td>120401325X</td>\n",
       "      <td>christina</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>They look good and stick good! I just don't li...</td>\n",
       "      <td>4</td>\n",
       "      <td>Looks Good</td>\n",
       "      <td>1400630400</td>\n",
       "      <td>05 21, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASY55RVNIL0UD</td>\n",
       "      <td>120401325X</td>\n",
       "      <td>emily l.</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>These stickers work like the review says they ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Really great product.</td>\n",
       "      <td>1389657600</td>\n",
       "      <td>01 14, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2TMXE2AFO7ONB</td>\n",
       "      <td>120401325X</td>\n",
       "      <td>Erica</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>These are awesome and make my phone look so st...</td>\n",
       "      <td>5</td>\n",
       "      <td>LOVE LOVE LOVE</td>\n",
       "      <td>1403740800</td>\n",
       "      <td>06 26, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AWJ0WZQYMYFQ4</td>\n",
       "      <td>120401325X</td>\n",
       "      <td>JM</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>Item arrived in great time and was in perfect ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Cute!</td>\n",
       "      <td>1382313600</td>\n",
       "      <td>10 21, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATX7CZYFXI1KW</td>\n",
       "      <td>120401325X</td>\n",
       "      <td>patrice m rogoza</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>awesome! stays on, and looks great. can be use...</td>\n",
       "      <td>5</td>\n",
       "      <td>leopard home button sticker for iphone 4s</td>\n",
       "      <td>1359849600</td>\n",
       "      <td>02 3, 2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin      reviewerName helpful  \\\n",
       "0  A30TL5EWN6DFXT  120401325X         christina  [0, 0]   \n",
       "1   ASY55RVNIL0UD  120401325X          emily l.  [0, 0]   \n",
       "2  A2TMXE2AFO7ONB  120401325X             Erica  [0, 0]   \n",
       "3   AWJ0WZQYMYFQ4  120401325X                JM  [4, 4]   \n",
       "4   ATX7CZYFXI1KW  120401325X  patrice m rogoza  [2, 3]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  They look good and stick good! I just don't li...        4   \n",
       "1  These stickers work like the review says they ...        5   \n",
       "2  These are awesome and make my phone look so st...        5   \n",
       "3  Item arrived in great time and was in perfect ...        4   \n",
       "4  awesome! stays on, and looks great. can be use...        5   \n",
       "\n",
       "                                     summary  unixReviewTime   reviewTime  \n",
       "0                                 Looks Good      1400630400  05 21, 2014  \n",
       "1                      Really great product.      1389657600  01 14, 2014  \n",
       "2                             LOVE LOVE LOVE      1403740800  06 26, 2014  \n",
       "3                                      Cute!      1382313600  10 21, 2013  \n",
       "4  leopard home button sticker for iphone 4s      1359849600   02 3, 2013  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad61cfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['reviewerID', 'asin', 'reviewerName', 'helpful', 'reviewText',\n",
       "       'overall', 'summary', 'unixReviewTime', 'reviewTime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c6ec6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They look good and stick good! I just don't like the rounded shape because I was always bumping it and Siri kept popping up and it was irritating. I just won't buy a product like this again\n"
     ]
    }
   ],
   "source": [
    "# feature -  review text , label - overall rating and train our RNN i=over this daatset for NLP  task . \n",
    "\n",
    "text = df.reviewText[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e61ed0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they', 'look', 'good', 'and', 'stick', 'good', 'just', 'don', 'like', 'the', 'rounded', 'shape', 'because', 'was', 'always', 'bumping', 'it', 'and', 'siri', 'kept', 'popping', 'up', 'and', 'it', 'was', 'irritating', 'just', 'won', 'buy', 'product', 'like', 'this', 'again']\n"
     ]
    }
   ],
   "source": [
    "# Use gensim.utils.simple_preprocess() -> to preprocess a text and drop trailing spaces , puncuation marks and redundant words \n",
    "\n",
    "# (like am , a , was , the , etc...) for better training for RNN similar to LSTM /GRU \n",
    "\n",
    "# Here , we are actually Tokenizing a sentence \n",
    "\n",
    "# Tokenization - Breaking a sentence into its constituent Tokens without puncuation / trailing spaces for better training \n",
    "# of RNN as words are passed one by one after going through Word2Vec model . \n",
    "\n",
    "print(gensim.utils.simple_preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "441cb830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tokenization function to whole col of pd DataFrame \n",
    "\n",
    "review_text_preprocess = df.reviewText.apply(gensim.utils.simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8db28c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [they, look, good, and, stick, good, just, don...\n",
       "1         [these, stickers, work, like, the, review, say...\n",
       "2         [these, are, awesome, and, make, my, phone, lo...\n",
       "3         [item, arrived, in, great, time, and, was, in,...\n",
       "4         [awesome, stays, on, and, looks, great, can, b...\n",
       "                                ...                        \n",
       "194434    [works, great, just, like, my, original, one, ...\n",
       "194435    [great, product, great, packaging, high, quali...\n",
       "194436    [this, is, great, cable, just, as, good, as, t...\n",
       "194437    [really, like, it, becasue, it, works, well, w...\n",
       "194438    [product, as, described, have, wasted, lot, of...\n",
       "Name: reviewText, Length: 194439, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text_preprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebf63ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert Word2Vec and get meanigful Word embeddings so as to show relation b/w words \n",
    "# we solve a FAKE PROBLEM of 'Fill in the Blanks' using given dataset (sliding window used) and by several epochs \n",
    "# we get mcorrect weights (features values) of words in vocab. (word embedding  -> Word2Vec successful ) (ytrue = one hot enc.d)\n",
    "# to solve this RNN task , we use gensim.models.Word2Vec (Vec is label and weights are feaures) \n",
    "\n",
    "rnn_word2vec = gensim.models.Word2Vec(\n",
    "    \n",
    "    window = 10 , # sliding window size \n",
    "    min_count = 2 , # min no. of words in sentence \n",
    "    workers  = 4  # CPU threads used for training \n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9875ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a vocab of words \n",
    "\n",
    "vocab = rnn_word2vec.build_vocab(review_text_preprocess , progress_per = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3741ef15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_word2vec.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6440ceb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61508618, 83868975)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_word2vec.train(review_text_preprocess  , total_examples = rnn_word2vec.corpus_count , epochs = rnn_word2vec.epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f9836a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model and use it later (.model extension)\n",
    "\n",
    "rnn_word2vec.save(\"./amazon_reviews.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b82da32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decent', 0.8175770044326782),\n",
       " ('great', 0.7860682606697083),\n",
       " ('nice', 0.7026644349098206),\n",
       " ('fantastic', 0.6905131936073303),\n",
       " ('excellent', 0.6373535990715027),\n",
       " ('outstanding', 0.6171596050262451),\n",
       " ('superb', 0.6157050728797913),\n",
       " ('awesome', 0.6103843450546265),\n",
       " ('exceptional', 0.6082326173782349),\n",
       " ('terrific', 0.5919394493103027)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we get predictions as Vector form of words ( simialr words have similar vwctor values in word embeddings )\n",
    "\n",
    "rnn_word2vec.wv.most_similar('good') # now , it started learning our English language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e781cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.03976939"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get similarity score between words \n",
    "\n",
    "rnn_word2vec.wv.similarity(w1 = 'good' , w2 = 'product')   # positive similarity - means highly correlated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4d52ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_embeddings = rnn_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cad21514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194439"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = review_text_preprocess\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20575c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.overall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae6262a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "labels = LabelEncoder()\n",
    "y_enc = labels.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4825747e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 4, ..., 4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "395c8d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_16308\\1163842542.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_word2vec = np.array([\n"
     ]
    }
   ],
   "source": [
    "# word2vec_embeddings = Word2Vec.load(\"your_word2vec_model_path\")  # Replace with your actual path\n",
    "\n",
    "# Map words to Word2Vec embeddings\n",
    "embedding_dim = word2vec_embeddings.vector_size\n",
    "X_word2vec = np.array([\n",
    "    [word2vec_embeddings.wv[word] if word in word2vec_embeddings.wv else np.zeros(embedding_dim) for word in sequence]\n",
    "    for sequence in X\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bff7f3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_word2vec.shape\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "998d1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2a30256",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_padded = pad_sequences(X_word2vec[:12000], maxlen= 30, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "187067bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b558548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_enc[:12000], test_size=0.2)\n",
    "                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ed6885f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of 0         4\n",
       "1         5\n",
       "2         5\n",
       "3         4\n",
       "4         5\n",
       "         ..\n",
       "194434    5\n",
       "194435    5\n",
       "194436    5\n",
       "194437    5\n",
       "194438    5\n",
       "Name: overall, Length: 194439, dtype: int64>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "867691da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d658fe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding , LSTM , Dense \n",
    "# Many to One RNN\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(LSTM(units = 128 , input_shape = (30 , embedding_dim) ))  # using LSTM  RNN model\n",
    "model.add(Dense(units = 64 , activation = 'softmax') )\n",
    "model.add(Dense(units = 5 , activation = 'softmax') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62399f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36c1d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "270/270 [==============================] - 12s 34ms/step - loss: 1.4059 - accuracy: 0.5122 - val_loss: 1.3560 - val_accuracy: 0.5125\n",
      "Epoch 2/20\n",
      "270/270 [==============================] - 8s 31ms/step - loss: 1.3302 - accuracy: 0.5188 - val_loss: 1.3399 - val_accuracy: 0.5125\n",
      "Epoch 3/20\n",
      "270/270 [==============================] - 8s 31ms/step - loss: 1.3227 - accuracy: 0.5188 - val_loss: 1.3385 - val_accuracy: 0.5125\n",
      "Epoch 4/20\n",
      " 75/270 [=======>......................] - ETA: 5s - loss: 1.3419 - accuracy: 0.5021"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs= 20, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a995892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 1s 9ms/step - loss: 2.9282 - accuracy: 0.5142\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "492c8ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5141666531562805"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6c2ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vectors(sentence, word2vec_model):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [word2vec_model.wv[word] if word in word2vec_model.wv else np.zeros(100) for word in words]\n",
    "    return np.array(word_vectors)\n",
    "\n",
    "\n",
    "def predict_sentiment(sentence, model, word2vec_model, max_sequence_length):\n",
    "    # Convert sentence to Word2Vec vectors\n",
    "    input_sequence = sentence_to_vectors(sentence, word2vec_model)\n",
    "\n",
    "    # Pad the input sequence\n",
    "    padded_input_sequence = pad_sequences([input_sequence], maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(padded_input_sequence)\n",
    "\n",
    "    # Get the predicted class\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "    return predicted_class, predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70a8f982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 700ms/step\n",
      "rating = 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "out1 , out2  = predict_sentiment('terrible product', model,rnn_word2vec , 30)\n",
    "\n",
    "# out2 is correct output \n",
    "index = out1\n",
    "\n",
    "print(f\"rating = {index}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
